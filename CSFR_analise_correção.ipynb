{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d43fe9a2",
   "metadata": {},
   "source": [
    "# **Bloco 1: Importação de Bibliotecas e Carregamento dos Dados**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12f9aa56",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instalação de pacotes necessários\n",
    "%pip install pandas scikit-learn nltk openpyxl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2ea34060",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-11 13:33:16,225 - INFO - Stopwords do NLTK baixadas com sucesso\n",
      "2025-04-11 13:33:17,325 - INFO - Arquivo carregado com sucesso. Dimensões: (2571, 55)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total de linhas: 2571\n",
      "Total de colunas: 55\n",
      "Colunas no DataFrame: Classificação, Sub-Classificação, Projeto, Contrato Juridico, Contrato SAP, Item, Linha, Descrição Linha de Serviço, Unidade de Medida, Qtde Prevista, Qtde Realizada, Preço Base, Valor Líquido Realizado, Valor PIS, Valor COFINS, Valor ICMS, Valor Bruto Realizado, Fornecedor, Data Base, Moeda, idLinhaServico, Indice1, Peso1, FatorK1, Indice2, Peso2, FatorK2, Indice3, FatorK3, Peso3, Indice4, Peso4, FatorK4, Indice5, Peso5, FatorK5, Indice6, Peso6, FatorK6, Análise.idLinhaServico, ACG, EQN, IGP, INS, IPAMB, IPAMQ, IPAOG, IPAPQ, IPCA, IPI, IPM, MME, MOE, TPO, Total\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.multioutput import MultiOutputClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from openpyxl import Workbook\n",
    "from openpyxl.styles import PatternFill\n",
    "import logging\n",
    "\n",
    "# Configurar logging para acompanhar o processo\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "\n",
    "# Baixar stopwords do NLTK (necessário na primeira execução)\n",
    "try:\n",
    "    nltk.download('stopwords', quiet=True)\n",
    "    logging.info(\"Stopwords do NLTK baixadas com sucesso\")\n",
    "except Exception as e:\n",
    "    logging.warning(f\"Erro ao baixar stopwords: {e}\")\n",
    "\n",
    "# Caminho do arquivo com formato padronizado\n",
    "file_path = r'C:\\\\Users\\\\FUDO\\\\OneDrive - PETROBRAS\\\\Documentos\\\\Python\\\\Problema do Leandro\\\\ContratoServico-FormulasR 1.xlsx'\n",
    "\n",
    "# Verificar se o arquivo existe\n",
    "if not os.path.exists(file_path):\n",
    "    logging.error(f\"Arquivo não encontrado: {file_path}\")\n",
    "    raise FileNotFoundError(f\"O arquivo {file_path} não existe\")\n",
    "\n",
    "# Carregamento dos dados com tratamento de erro\n",
    "try:\n",
    "    df = pd.read_excel(file_path)\n",
    "    logging.info(f\"Arquivo carregado com sucesso. Dimensões: {df.shape}\")\n",
    "except Exception as e:\n",
    "    logging.error(f\"Erro ao carregar o arquivo: {e}\")\n",
    "    raise\n",
    "\n",
    "# Exibir informações básicas sobre o dataframe\n",
    "print(f\"Total de linhas: {df.shape[0]}\")\n",
    "print(f\"Total de colunas: {df.shape[1]}\")\n",
    "print(f\"Colunas no DataFrame: {', '.join(df.columns)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eceeaa24",
   "metadata": {},
   "source": [
    "# **Bloco 2: Análise Exploratória e Pré-processamento das Features**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "68e8183f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-11 13:33:24,895 - INFO - Valores nulos na coluna 'Descrição Linha de Serviço' preenchidos com ''\n",
      "2025-04-11 13:33:24,899 - INFO - Valores nulos na coluna 'Classificação' preenchidos com ''\n",
      "2025-04-11 13:33:24,901 - INFO - Valores nulos na coluna 'Fornecedor' preenchidos com ''\n",
      "2025-04-11 13:33:24,903 - INFO - Valores nulos na coluna 'Moeda' preenchidos com ''\n",
      "2025-04-11 13:33:24,904 - INFO - Valores nulos na coluna 'Item' preenchidos com 0\n",
      "2025-04-11 13:33:24,999 - INFO - TF-IDF aplicado com sucesso. Dimensões: (2571, 50)\n",
      "2025-04-11 13:33:25,014 - INFO - Codificação One-Hot aplicada com sucesso. Dimensões: (2571, 41)\n",
      "2025-04-11 13:33:25,016 - INFO - Coluna 'Item' convertida para numérico\n",
      "2025-04-11 13:33:25,020 - INFO - DataFrame de features combinado. Dimensões: (2571, 92)\n",
      "2025-04-11 13:33:25,027 - INFO - Linhas com valores para treino: 1221\n",
      "2025-04-11 13:33:25,028 - INFO - Linhas para previsão: 1350\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Quantidade de valores nulos por coluna:\n",
      "Classificação                    0\n",
      "Sub-Classificação                6\n",
      "Projeto                          6\n",
      "Contrato Juridico                2\n",
      "Contrato SAP                     0\n",
      "Item                             0\n",
      "Linha                            0\n",
      "Descrição Linha de Serviço       0\n",
      "Unidade de Medida                0\n",
      "Qtde Prevista                    0\n",
      "Qtde Realizada                   0\n",
      "Preço Base                       0\n",
      "Valor Líquido Realizado          0\n",
      "Valor PIS                        0\n",
      "Valor COFINS                     0\n",
      "Valor ICMS                       0\n",
      "Valor Bruto Realizado            0\n",
      "Fornecedor                       0\n",
      "Data Base                        0\n",
      "Moeda                            0\n",
      "idLinhaServico                   0\n",
      "Indice1                       1350\n",
      "Peso1                         1350\n",
      "FatorK1                       1350\n",
      "Indice2                       1413\n",
      "Peso2                         1413\n",
      "FatorK2                       1413\n",
      "Indice3                       1812\n",
      "FatorK3                       1812\n",
      "Peso3                         1812\n",
      "Indice4                       2284\n",
      "Peso4                         2284\n",
      "FatorK4                       2284\n",
      "Indice5                       2551\n",
      "Peso5                         2551\n",
      "FatorK5                       2551\n",
      "Indice6                       2551\n",
      "Peso6                         2551\n",
      "FatorK6                       2551\n",
      "Análise.idLinhaServico        1350\n",
      "ACG                           1904\n",
      "EQN                           2367\n",
      "IGP                           2076\n",
      "INS                           2361\n",
      "IPAMB                         2222\n",
      "IPAMQ                         1766\n",
      "IPAOG                         2570\n",
      "IPAPQ                         2551\n",
      "IPCA                          2293\n",
      "IPI                           2520\n",
      "IPM                           2566\n",
      "MME                           2520\n",
      "MOE                           2345\n",
      "TPO                           2468\n",
      "Total                            0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Verificar valores nulos nas colunas relevantes\n",
    "print(\"\\nQuantidade de valores nulos por coluna:\")\n",
    "print(df.isnull().sum())\n",
    "\n",
    "# Selecionar colunas relevantes\n",
    "features = ['Descrição Linha de Serviço', 'Classificação', 'Fornecedor', 'Moeda', 'Item']\n",
    "targets = ['Indice1', 'Peso1', 'FatorK1', 'Indice2', 'Peso2', 'FatorK2', \n",
    "           'Indice3', 'Peso3', 'FatorK3', 'Indice4', 'Peso4', 'FatorK4', \n",
    "           'Indice5', 'Peso5', 'FatorK5', 'Indice6', 'Peso6', 'FatorK6']\n",
    "\n",
    "# Verificar se todas as colunas existem no DataFrame\n",
    "missing_cols = [col for col in features + targets if col not in df.columns]\n",
    "if missing_cols:\n",
    "    logging.warning(f\"Colunas não encontradas no DataFrame: {missing_cols}\")\n",
    "\n",
    "# Tratar valores nulos nas features de texto e categóricas\n",
    "for col in features:\n",
    "    if col in df.columns:\n",
    "        if df[col].dtype == 'object':\n",
    "            df[col] = df[col].fillna('')\n",
    "            logging.info(f\"Valores nulos na coluna '{col}' preenchidos com ''\")\n",
    "        else:\n",
    "            df[col] = df[col].fillna(0)\n",
    "            logging.info(f\"Valores nulos na coluna '{col}' preenchidos com 0\")\n",
    "\n",
    "# Pré-processamento de texto para 'Descrição Linha de Serviço'\n",
    "try:\n",
    "    stop_words = set(stopwords.words('portuguese'))\n",
    "    tfidf = TfidfVectorizer(stop_words=list(stop_words), max_features=50)\n",
    "    \n",
    "    # Garantir que não haja valores nulos antes do TF-IDF\n",
    "    if 'Descrição Linha de Serviço' in df.columns:\n",
    "        desc_tfidf = tfidf.fit_transform(df['Descrição Linha de Serviço']).toarray()\n",
    "        desc_df = pd.DataFrame(desc_tfidf, columns=[f\"tfidf_{i}\" for i in range(desc_tfidf.shape[1])])\n",
    "        logging.info(f\"TF-IDF aplicado com sucesso. Dimensões: {desc_df.shape}\")\n",
    "    else:\n",
    "        logging.error(\"Coluna 'Descrição Linha de Serviço' não encontrada no DataFrame\")\n",
    "        desc_df = pd.DataFrame()\n",
    "except Exception as e:\n",
    "    logging.error(f\"Erro ao processar TF-IDF: {e}\")\n",
    "    desc_df = pd.DataFrame()\n",
    "\n",
    "# Codificação One-Hot para variáveis categóricas com tratamento de erro\n",
    "try:\n",
    "    cat_cols = [col for col in ['Classificação', 'Fornecedor', 'Moeda'] if col in df.columns]\n",
    "    if cat_cols:\n",
    "        # Remover valores NaN ou vazios antes da codificação\n",
    "        df_cat = df[cat_cols].fillna('MISSING')\n",
    "        \n",
    "        encoder = OneHotEncoder(sparse_output=False, handle_unknown='ignore')\n",
    "        cat_encoded = encoder.fit_transform(df_cat)\n",
    "        cat_df = pd.DataFrame(cat_encoded, \n",
    "                            columns=encoder.get_feature_names_out(cat_cols))\n",
    "        logging.info(f\"Codificação One-Hot aplicada com sucesso. Dimensões: {cat_df.shape}\")\n",
    "    else:\n",
    "        logging.warning(\"Nenhuma coluna categórica encontrada para codificação\")\n",
    "        cat_df = pd.DataFrame()\n",
    "except Exception as e:\n",
    "    logging.error(f\"Erro na codificação One-Hot: {e}\")\n",
    "    cat_df = pd.DataFrame(index=range(len(df)))\n",
    "\n",
    "# Tratar 'Item' como numérico com verificação\n",
    "try:\n",
    "    if 'Item' in df.columns:\n",
    "        item_df = pd.DataFrame(df['Item'].fillna(0).astype(float), columns=['Item'])\n",
    "        logging.info(\"Coluna 'Item' convertida para numérico\")\n",
    "    else:\n",
    "        logging.warning(\"Coluna 'Item' não encontrada\")\n",
    "        item_df = pd.DataFrame(index=range(len(df)))\n",
    "except Exception as e:\n",
    "    logging.error(f\"Erro ao converter 'Item' para numérico: {e}\")\n",
    "    item_df = pd.DataFrame(index=range(len(df)))\n",
    "\n",
    "# Combinar todas as features em um único DataFrame\n",
    "X = pd.concat([desc_df, cat_df, item_df], axis=1)\n",
    "logging.info(f\"DataFrame de features combinado. Dimensões: {X.shape}\")\n",
    "\n",
    "# Preparar os alvos (substituir vazios por NaN)\n",
    "y = df[targets].replace('', np.nan)\n",
    "\n",
    "# Identificar linhas com pelo menos um valor não-nulo nos targets (para treino)\n",
    "# Alterado para identificar linhas com pelo menos um valor não nulo nos targets\n",
    "has_target_values = ~y.isna().all(axis=1)\n",
    "complete_rows = y[has_target_values].index\n",
    "incomplete_rows = y[~has_target_values].index\n",
    "\n",
    "logging.info(f\"Linhas com valores para treino: {len(complete_rows)}\")\n",
    "logging.info(f\"Linhas para previsão: {len(incomplete_rows)}\")\n",
    "\n",
    "# Verificar se há dados suficientes para treino\n",
    "min_train_samples = 10  # Definir um limiar mínimo para treinamento\n",
    "if len(complete_rows) < min_train_samples:\n",
    "    logging.warning(f\"Dados insuficientes para treinamento: {len(complete_rows)} < {min_train_samples}\")\n",
    "    print(f\"ATENÇÃO: Apenas {len(complete_rows)} amostras para treinamento. Os resultados podem não ser confiáveis.\")\n",
    "\n",
    "# Separar dados de treino e teste\n",
    "X_train = X.loc[complete_rows]\n",
    "y_train = y.loc[complete_rows].fillna({'FatorK1': 1.0, 'FatorK2': 1.0, 'FatorK3': 1.0, \n",
    "                                      'FatorK4': 1.0, 'FatorK5': 1.0, 'FatorK6': 1.0})\n",
    "X_pred = X.loc[incomplete_rows] if len(incomplete_rows) > 0 else pd.DataFrame()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31f03f37",
   "metadata": {},
   "source": [
    "# **Bloco 3: Treinamento do Modelo de Machine Learning**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f9fe42b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-11 13:34:15,101 - INFO - Validação cruzada para índices: 0.4056 (±0.2312)\n",
      "2025-04-11 13:34:15,102 - INFO - Modelo de classificação para índices treinado com sucesso\n",
      "2025-04-11 13:34:17,237 - INFO - Validação cruzada para pesos: -3.5357 (±4.6310)\n",
      "2025-04-11 13:34:17,239 - INFO - Modelo de regressão para pesos treinado com sucesso\n",
      "2025-04-11 13:34:17,314 - INFO - Previsão de índices concluída\n",
      "2025-04-11 13:34:17,351 - INFO - Previsão de pesos concluída e normalizada\n",
      "2025-04-11 13:34:17,359 - INFO - Atribuído índice padrão 'IGP' para 108 linhas com Peso2 > 0 sem índice\n",
      "2025-04-11 13:34:17,362 - INFO - Zerados 336 pesos onde Indice2 está vazio ou é 'Nenhum'\n",
      "2025-04-11 13:34:17,366 - INFO - Atribuído índice padrão 'IGP' para 296 linhas com Peso3 > 0 sem índice\n",
      "2025-04-11 13:34:17,368 - INFO - Zerados 916 pesos onde Indice3 está vazio ou é 'Nenhum'\n",
      "2025-04-11 13:34:17,371 - INFO - Atribuído índice padrão 'IGP' para 96 linhas com Peso4 > 0 sem índice\n",
      "2025-04-11 13:34:17,379 - INFO - Zerados 1162 pesos onde Indice4 está vazio ou é 'Nenhum'\n",
      "2025-04-11 13:34:17,382 - INFO - Atribuído índice padrão 'IGP' para 29 linhas com Peso5 > 0 sem índice\n",
      "2025-04-11 13:34:17,384 - INFO - Zerados 1303 pesos onde Indice5 está vazio ou é 'Nenhum'\n",
      "2025-04-11 13:34:17,386 - INFO - Atribuído índice padrão 'IGP' para 29 linhas com Peso6 > 0 sem índice\n",
      "2025-04-11 13:34:17,390 - INFO - Zerados 1303 pesos onde Indice6 está vazio ou é 'Nenhum'\n",
      "2025-04-11 13:34:17,451 - INFO - Pesos renormalizados para linha 2124 após correções de consistência\n",
      "2025-04-11 13:34:17,453 - INFO - Pesos renormalizados para linha 2131 após correções de consistência\n",
      "2025-04-11 13:34:17,459 - INFO - Pesos renormalizados para linha 2140 após correções de consistência\n",
      "2025-04-11 13:34:17,462 - INFO - Pesos renormalizados para linha 2141 após correções de consistência\n",
      "2025-04-11 13:34:17,467 - INFO - Pesos renormalizados para linha 2147 após correções de consistência\n",
      "2025-04-11 13:34:17,470 - INFO - Pesos renormalizados para linha 2148 após correções de consistência\n",
      "2025-04-11 13:34:17,484 - INFO - Pesos renormalizados para linha 2542 após correções de consistência\n",
      "2025-04-11 13:34:17,486 - INFO - Pesos renormalizados para linha 2543 após correções de consistência\n",
      "2025-04-11 13:34:17,490 - INFO - Pesos renormalizados para linha 2550 após correções de consistência\n",
      "2025-04-11 13:34:17,495 - INFO - Pesos renormalizados para linha 2551 após correções de consistência\n",
      "2025-04-11 13:34:17,499 - INFO - DataFrame de previsões criado com sucesso\n"
     ]
    }
   ],
   "source": [
    "# Separar índices e pesos para modelagem separada\n",
    "indices_cols = [col for col in targets if 'Indice' in col]\n",
    "pesos_cols = [col for col in targets if 'Peso' in col]\n",
    "fatork_cols = [col for col in targets if 'FatorK' in col]\n",
    "\n",
    "# Função para verificar a qualidade do modelo usando validação cruzada\n",
    "def evaluate_model(model, X, y, model_type=\"classificação\"):\n",
    "    if len(X) >= 10:  # Verificar se há amostras suficientes para CV\n",
    "        try:\n",
    "            scores = cross_val_score(model, X, y, cv=min(5, len(X)))\n",
    "            logging.info(f\"Validação cruzada para {model_type}: {scores.mean():.4f} (±{scores.std():.4f})\")\n",
    "            return scores.mean()\n",
    "        except Exception as e:\n",
    "            logging.warning(f\"Erro na validação cruzada do modelo de {model_type}: {e}\")\n",
    "            return None\n",
    "    else:\n",
    "        logging.warning(f\"Amostras insuficientes para validação cruzada do modelo de {model_type}\")\n",
    "        return None\n",
    "\n",
    "# Treinar modelos apenas se houver dados suficientes\n",
    "if len(complete_rows) >= min_train_samples:\n",
    "    try:\n",
    "        # Modelo para prever índices (classificação)\n",
    "        # Filtrar apenas as linhas que têm pelo menos um índice não-nulo\n",
    "        indices_data = y_train[indices_cols].fillna('Nenhum')\n",
    "        has_indices = (indices_data != 'Nenhum').any(axis=1)\n",
    "        \n",
    "        if has_indices.any():\n",
    "            clf = MultiOutputClassifier(RandomForestClassifier(n_estimators=100, random_state=42))\n",
    "            clf.fit(X_train[has_indices], indices_data[has_indices])\n",
    "            evaluate_model(clf, X_train[has_indices], indices_data[has_indices], \"índices\")\n",
    "            logging.info(\"Modelo de classificação para índices treinado com sucesso\")\n",
    "        else:\n",
    "            clf = None\n",
    "            logging.warning(\"Nenhum dado válido para treinar o modelo de índices\")\n",
    "        \n",
    "        # Modelo para prever pesos (regressão)\n",
    "        # Filtrar apenas as linhas que têm pelo menos um peso não-nulo\n",
    "        pesos_data = y_train[pesos_cols].fillna(0)\n",
    "        has_pesos = (pesos_data > 0).any(axis=1)\n",
    "        \n",
    "        if has_pesos.any():\n",
    "            reg = RandomForestRegressor(n_estimators=100, random_state=42)\n",
    "            reg.fit(X_train[has_pesos], pesos_data[has_pesos])\n",
    "            evaluate_model(reg, X_train[has_pesos], pesos_data[has_pesos], \"pesos\")\n",
    "            logging.info(\"Modelo de regressão para pesos treinado com sucesso\")\n",
    "        else:\n",
    "            reg = None\n",
    "            logging.warning(\"Nenhum dado válido para treinar o modelo de pesos\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        logging.error(f\"Erro no treinamento dos modelos: {e}\")\n",
    "        clf, reg = None, None\n",
    "else:\n",
    "    logging.warning(\"Dados insuficientes para treinar modelos\")\n",
    "    clf, reg = None, None\n",
    "\n",
    "# Prever índices e pesos para linhas incompletas (se houver modelos treinados e dados para prever)\n",
    "if len(incomplete_rows) > 0 and X_pred.shape[0] > 0:\n",
    "    # Inicializar DataFrame de previsões com valores vazios\n",
    "    pred_df = pd.DataFrame(index=incomplete_rows, columns=targets)\n",
    "    \n",
    "    # Prever índices se o modelo foi treinado\n",
    "    if clf is not None:\n",
    "        try:\n",
    "            indices_pred = clf.predict(X_pred)\n",
    "            for i, col in enumerate(indices_cols):\n",
    "                pred_df[col] = indices_pred[:, i]\n",
    "            logging.info(\"Previsão de índices concluída\")\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Erro na previsão de índices: {e}\")\n",
    "            # Usar 'Nenhum' como fallback para índices\n",
    "            for col in indices_cols:\n",
    "                pred_df[col] = 'Nenhum'\n",
    "    else:\n",
    "        # Usar 'Nenhum' como fallback para índices se não há modelo\n",
    "        for col in indices_cols:\n",
    "            pred_df[col] = 'Nenhum'\n",
    "    \n",
    "    # Prever pesos se o modelo foi treinado\n",
    "    if reg is not None:\n",
    "        try:\n",
    "            pesos_pred = reg.predict(X_pred)\n",
    "            \n",
    "            # Normalizar pesos para soma <=\n",
    "            def normalize_weights(weights):\n",
    "                # Substituir NaN com 0 para evitar problemas\n",
    "                weights = np.nan_to_num(weights)\n",
    "                total = np.sum(weights)\n",
    "                if total > 1:\n",
    "                    # Normalizar apenas valores não-nulos\n",
    "                    return weights / total if total > 0 else weights\n",
    "                return weights\n",
    "            \n",
    "            pesos_pred_normalized = np.apply_along_axis(normalize_weights, 1, pesos_pred)\n",
    "            \n",
    "            for i, col in enumerate(pesos_cols):\n",
    "                pred_df[col] = pesos_pred_normalized[:, i]\n",
    "            logging.info(\"Previsão de pesos concluída e normalizada\")\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Erro na previsão de pesos: {e}\")\n",
    "            # Usar 0 como fallback para pesos\n",
    "            for col in pesos_cols:\n",
    "                pred_df[col] = 0\n",
    "    else:\n",
    "        # Usar 0 como fallback para pesos se não há modelo\n",
    "        for col in pesos_cols:\n",
    "            pred_df[col] = 0\n",
    "            \n",
    "    # Garantir consistência entre índices e pesos\n",
    "    # Se um peso for > 0, mas não há índice, atribuir um índice padrão\n",
    "    # Se um índice for 'Nenhum' ou vazio, zerar o peso correspondente\n",
    "    for i in range(1, 7):  # Para cada par índice/peso (1 a 6)\n",
    "        indice_col = f'Indice{i}'\n",
    "        peso_col = f'Peso{i}'\n",
    "        \n",
    "        if indice_col in pred_df.columns and peso_col in pred_df.columns:\n",
    "            # Caso 1: Peso > 0 mas sem índice - atribuir índice padrão \"IGP\"\n",
    "            mask = (pred_df[peso_col] > 0) & ((pred_df[indice_col] == '') | (pred_df[indice_col] == 'Nenhum'))\n",
    "            if mask.any():\n",
    "                pred_df.loc[mask, indice_col] = \"IGP\"  # Índice padrão\n",
    "                logging.info(f\"Atribuído índice padrão 'IGP' para {mask.sum()} linhas com {peso_col} > 0 sem índice\")\n",
    "            \n",
    "            # Caso 2: Índice vazio ou 'Nenhum', zerar o peso correspondente\n",
    "            mask = ((pred_df[indice_col] == '') | (pred_df[indice_col] == 'Nenhum'))\n",
    "            if mask.any():\n",
    "                pred_df.loc[mask, peso_col] = 0\n",
    "                logging.info(f\"Zerados {mask.sum()} pesos onde {indice_col} está vazio ou é 'Nenhum'\")\n",
    "    \n",
    "    # Verificar novamente se a soma dos pesos é <= 1 após as correções\n",
    "    for idx in pred_df.index:\n",
    "        peso_values = [pred_df.loc[idx, f'Peso{i}'] for i in range(1, 7)]\n",
    "        peso_values = [p for p in peso_values if not pd.isna(p)]\n",
    "        total = sum(peso_values)\n",
    "        \n",
    "        if total > 1:\n",
    "            for i in range(1, 7):\n",
    "                peso_col = f'Peso{i}'\n",
    "                if peso_col in pred_df.columns and not pd.isna(pred_df.loc[idx, peso_col]) and pred_df.loc[idx, peso_col] > 0:\n",
    "                    pred_df.loc[idx, peso_col] = pred_df.loc[idx, peso_col] / total\n",
    "            \n",
    "            logging.info(f\"Pesos renormalizados para linha {idx} após correções de consistência\")\n",
    "    \n",
    "    # Definir FatorK como 1.0 para todas as linhas\n",
    "    for col in fatork_cols:\n",
    "        pred_df[col] = 1.0\n",
    "        \n",
    "    logging.info(\"DataFrame de previsões criado com sucesso\")\n",
    "else:\n",
    "    pred_df = pd.DataFrame()\n",
    "    logging.info(\"Nenhuma linha para previsão ou modelos não treinados\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14295ee3",
   "metadata": {},
   "source": [
    "# **Bloco 4: Combinar Previsões com Dados Originais**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1a9c638f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-11 13:34:43,156 - INFO - Previsões combinadas com dados originais\n"
     ]
    }
   ],
   "source": [
    "# Copiar o DataFrame original\n",
    "df_filled = df.copy()\n",
    "\n",
    "# Flag para verificar se houve previsões aplicadas\n",
    "predictions_applied = False\n",
    "\n",
    "# Registrar quais células foram preenchidas (para colorir no Excel depois)\n",
    "filled_cells = {}  # Dicionário para armazenar posições (linha, coluna) das células preenchidas\n",
    "\n",
    "# Atualizar apenas as linhas incompletas com as previsões (se houver)\n",
    "if not pred_df.empty:\n",
    "    for idx in pred_df.index:\n",
    "        # Primeiro verificar se precisamos aplicar qualquer previsão nesta linha\n",
    "        any_missing = False\n",
    "        for col in targets:\n",
    "            is_empty = pd.isna(df_filled.loc[idx, col]) or df_filled.loc[idx, col] == ''\n",
    "            if is_empty:\n",
    "                any_missing = True\n",
    "                break\n",
    "        \n",
    "        if any_missing:\n",
    "            # Primeiro preenchimento: encontrar índices válidos e seus pesos correspondentes\n",
    "            valid_indices_and_weights = []\n",
    "            \n",
    "            # Verificar valores existentes (não nulos e não vazios) nos dados originais\n",
    "            for i in range(1, 7):\n",
    "                indice_col = f'Indice{i}'\n",
    "                peso_col = f'Peso{i}'\n",
    "                indice_value = df_filled.loc[idx, indice_col]\n",
    "                peso_value = df_filled.loc[idx, peso_col]\n",
    "                \n",
    "                indice_filled = not (pd.isna(indice_value) or indice_value == '' or indice_value == 'Nenhum')\n",
    "                peso_filled = not (pd.isna(peso_value) or peso_value == 0)\n",
    "                \n",
    "                # Adicionar à lista se já estiver preenchido\n",
    "                if indice_filled or peso_filled:\n",
    "                    valid_indices_and_weights.append((indice_col, peso_col))\n",
    "            \n",
    "            # Agora, preencher os valores ausentes\n",
    "            for i in range(1, 7):\n",
    "                indice_col = f'Indice{i}'\n",
    "                peso_col = f'Peso{i}'\n",
    "                fatork_col = f'FatorK{i}'\n",
    "                \n",
    "                # Verificar se o par de índice e peso é válido para atualizar\n",
    "                if (indice_col, peso_col) not in valid_indices_and_weights:\n",
    "                    # Verificar se existem valores nas previsões\n",
    "                    indice_pred = pred_df.loc[idx, indice_col] if indice_col in pred_df.columns else None\n",
    "                    peso_pred = pred_df.loc[idx, peso_col] if peso_col in pred_df.columns else None\n",
    "                    \n",
    "                    # Só preencher se tiver previsão com valor não nulo/vazio\n",
    "                    valid_indice = indice_pred is not None and indice_pred != '' and indice_pred != 'Nenhum'\n",
    "                    valid_peso = peso_pred is not None and peso_pred > 0\n",
    "                    \n",
    "                    # Aplicar previsões apenas para pares válidos (índice e peso)\n",
    "                    if valid_indice and valid_peso:\n",
    "                        # Preencher índice\n",
    "                        is_empty_indice = pd.isna(df_filled.loc[idx, indice_col]) or df_filled.loc[idx, indice_col] == ''\n",
    "                        if is_empty_indice:\n",
    "                            if idx not in filled_cells:\n",
    "                                filled_cells[idx] = []\n",
    "                            filled_cells[idx].append(indice_col)\n",
    "                            df_filled.loc[idx, indice_col] = indice_pred\n",
    "                            predictions_applied = True\n",
    "                        \n",
    "                        # Preencher peso\n",
    "                        is_empty_peso = pd.isna(df_filled.loc[idx, peso_col]) or df_filled.loc[idx, peso_col] == 0\n",
    "                        if is_empty_peso:\n",
    "                            if idx not in filled_cells:\n",
    "                                filled_cells[idx] = []\n",
    "                            filled_cells[idx].append(peso_col)\n",
    "                            df_filled.loc[idx, peso_col] = peso_pred\n",
    "                            predictions_applied = True\n",
    "                        \n",
    "                        # Preencher fatorK se necessário\n",
    "                        is_empty_fatork = pd.isna(df_filled.loc[idx, fatork_col])\n",
    "                        if is_empty_fatork:\n",
    "                            if idx not in filled_cells:\n",
    "                                filled_cells[idx] = []\n",
    "                            filled_cells[idx].append(fatork_col)\n",
    "                            df_filled.loc[idx, fatork_col] = 1.0\n",
    "                            predictions_applied = True\n",
    "    \n",
    "    logging.info(\"Previsões combinadas com dados originais\")\n",
    "else:\n",
    "    logging.info(\"Nenhuma previsão para combinar com os dados originais\")\n",
    "\n",
    "# Garantir que os valores sejam consistentes (ex.: substituir 'Nenhum' por vazio nos índices)\n",
    "for col in indices_cols:\n",
    "    if col in df_filled.columns:\n",
    "        mask = df_filled[col] == 'Nenhum'\n",
    "        if mask.any():\n",
    "            df_filled.loc[mask, col] = ''\n",
    "            logging.info(f\"Valores 'Nenhum' na coluna {col} substituídos por vazio\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "771ee2da",
   "metadata": {},
   "source": [
    "# **Bloco 5: Exportar o Arquivo com Cores Diferenciadas**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0a5ba272",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-11 13:34:52,015 - INFO - Arquivo 'output_preenchido_corrigido.xlsx' gerado com sucesso!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Arquivo 'output_preenchido_corrigido.xlsx' gerado com sucesso!\n",
      "Total de 9189 células preenchidas em 1350 linhas.\n"
     ]
    }
   ],
   "source": [
    "# Definir nome do arquivo de saída\n",
    "output_file = 'output_preenchido_corrigido.xlsx'\n",
    "\n",
    "try:\n",
    "    # Criar um workbook e worksheet no openpyxl\n",
    "    wb = Workbook()\n",
    "    ws = wb.active\n",
    "    \n",
    "    # Definir cores para cada grupo\n",
    "    fills = {\n",
    "        'Indice1': PatternFill(start_color='ADD8E6', end_color='ADD8E6', fill_type='solid'),  # Azul claro\n",
    "        'Peso1': PatternFill(start_color='ADD8E6', end_color='ADD8E6', fill_type='solid'),\n",
    "        'FatorK1': PatternFill(start_color='ADD8E6', end_color='ADD8E6', fill_type='solid'),\n",
    "        'Indice2': PatternFill(start_color='90EE90', end_color='90EE90', fill_type='solid'),  # Verde claro\n",
    "        'Peso2': PatternFill(start_color='90EE90', end_color='90EE90', fill_type='solid'),\n",
    "        'FatorK2': PatternFill(start_color='90EE90', end_color='90EE90', fill_type='solid'),\n",
    "        'Indice3': PatternFill(start_color='FFFFE0', end_color='FFFFE0', fill_type='solid'),  # Amarelo claro\n",
    "        'Peso3': PatternFill(start_color='FFFFE0', end_color='FFFFE0', fill_type='solid'),\n",
    "        'FatorK3': PatternFill(start_color='FFFFE0', end_color='FFFFE0', fill_type='solid'),\n",
    "        'Indice4': PatternFill(start_color='FFA07A', end_color='FFA07A', fill_type='solid'),  # Laranja claro\n",
    "        'Peso4': PatternFill(start_color='FFA07A', end_color='FFA07A', fill_type='solid'),\n",
    "        'FatorK4': PatternFill(start_color='FFA07A', end_color='FFA07A', fill_type='solid'),\n",
    "        'Indice5': PatternFill(start_color='DDA0DD', end_color='DDA0DD', fill_type='solid'),  # Roxo claro\n",
    "        'Peso5': PatternFill(start_color='DDA0DD', end_color='DDA0DD', fill_type='solid'),\n",
    "        'FatorK5': PatternFill(start_color='DDA0DD', end_color='DDA0DD', fill_type='solid'),\n",
    "        'Indice6': PatternFill(start_color='F0E68C', end_color='F0E68C', fill_type='solid'),  # Amarelo ouro\n",
    "        'Peso6': PatternFill(start_color='F0E68C', end_color='F0E68C', fill_type='solid'),\n",
    "        'FatorK6': PatternFill(start_color='F0E68C', end_color='F0E68C', fill_type='solid'),\n",
    "    }\n",
    "    \n",
    "    # Escrever cabeçalhos\n",
    "    for col_num, value in enumerate(df_filled.columns, 1):\n",
    "        ws.cell(row=1, column=col_num, value=value)\n",
    "    \n",
    "    # Escrever dados e aplicar cores para valores preenchidos pelo modelo\n",
    "    for row_num, (idx, row) in enumerate(df_filled.iterrows(), 2):\n",
    "        for col_num, (col_name, value) in enumerate(row.items(), 1):\n",
    "            cell = ws.cell(row=row_num, column=col_num, value=value)\n",
    "            \n",
    "            # Verificar se essa célula foi preenchida pelo modelo\n",
    "            if idx in filled_cells and col_name in filled_cells[idx] and col_name in fills:\n",
    "                cell.fill = fills[col_name]\n",
    "    \n",
    "    # Salvar o arquivo\n",
    "    wb.save(output_file)\n",
    "    logging.info(f\"Arquivo '{output_file}' gerado com sucesso!\")\n",
    "    print(f\"Arquivo '{output_file}' gerado com sucesso!\")\n",
    "    \n",
    "    # Exibir estatísticas\n",
    "    if predictions_applied:\n",
    "        n_cells = sum(len(cols) for cols in filled_cells.values())\n",
    "        n_rows = len(filled_cells)\n",
    "        print(f\"Total de {n_cells} células preenchidas em {n_rows} linhas.\")\n",
    "    else:\n",
    "        print(\"Nenhuma célula foi preenchida pelo modelo.\")\n",
    "        \n",
    "except Exception as e:\n",
    "    logging.error(f\"Erro ao criar arquivo Excel: {e}\")\n",
    "    print(f\"Erro ao gerar o arquivo Excel: {e}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
